import os
import numpy as np
import pandas as pd
import tensorflow as tf
import joblib  # Import ƒë·ªÉ load scaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler

# ƒê·ªãnh nghƒ©a th∆∞ m·ª•c ƒë·ªông
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
SPLIT_DATA_DIR = os.path.join(BASE_DIR, "data", "splits")
MODEL_DIR = os.path.join(BASE_DIR, "models")
os.makedirs(MODEL_DIR, exist_ok=True)

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn scaler.pkl
SCALER_PATH = os.path.join(BASE_DIR, "data", "processed", "scaler.pkl")

# H√†m ki·ªÉm tra file t·ªìn t·∫°i
def check_file_exists(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"‚ùå File {file_path} kh√¥ng t·ªìn t·∫°i!")

# Load dataset
def load_data(file_path):
    check_file_exists(file_path)
    df = pd.read_csv(file_path)

    # Chuy·ªÉn ƒë·ªïi timestamp v√† s·∫Øp x·∫øp
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df = df.sort_values(by="timestamp")
    df = df.drop(columns=["timestamp"])  # Lo·∫°i b·ªè timestamp tr∆∞·ªõc khi train
    
    # X·ª≠ l√Ω gi√° tr·ªã NaN
    df = df.fillna(df.median(numeric_only=True))
    
    return df

# T·∫°o chu·ªói d·ªØ li·ªáu (sequence) cho m√¥ h√¨nh LSTM
def create_sequences(data, n_steps):
    X, y = [], []
    for i in range(len(data) - n_steps):
        X.append(data[i:i + n_steps])
        y.append(data[i + n_steps][-1])  # C·ªôt cu·ªëi c√πng l√† nh√£n d·ª± ƒëo√°n
    return np.array(X), np.array(y)

# Thi·∫øt l·∫≠p tham s·ªë
N_STEPS = 5  # S·ªë b∆∞·ªõc ƒë·ªÉ d·ª± ƒëo√°n ti·∫øp theo
BATCH_SIZE = 16
EPOCHS = 20

# Load scaler ƒë√£ ƒë∆∞·ª£c fit t·ª´ preprocessing
scaler = joblib.load(SCALER_PATH)

# Load v√† ti·ªÅn x·ª≠ l√Ω d·ªØ li·ªáu
train_file = os.path.join(SPLIT_DATA_DIR, "train_data.csv")
val_file = os.path.join(SPLIT_DATA_DIR, "val_data.csv")
test_file = os.path.join(SPLIT_DATA_DIR, "test_data.csv")

train_df = load_data(train_file)
val_df = load_data(val_file)
test_df = load_data(test_file)

# ƒê·∫£m b·∫£o train_df, val_df v√† test_df c√≥ c√°c c·ªôt gi·ªëng nh∆∞ l√∫c hu·∫•n luy·ªán scaler
expected_columns = scaler.feature_names_in_  # C·ªôt m√† scaler ƒë√£ h·ªçc

# L·ªçc ch·ªâ c√°c c·ªôt mong ƒë·ª£i
train_df = train_df[expected_columns]
val_df = val_df[expected_columns]
test_df = test_df[expected_columns]

# Chu·∫©n h√≥a d·ªØ li·ªáu b·∫±ng scaler ƒë√£ l∆∞u
scaled_train = scaler.transform(train_df)
scaled_val = scaler.transform(val_df)
scaled_test = scaler.transform(test_df)

# T·∫°o d·ªØ li·ªáu ƒë·∫ßu v√†o cho LSTM
X_train, y_train = create_sequences(scaled_train, N_STEPS)
X_val, y_val = create_sequences(scaled_val, N_STEPS)
X_test, y_test = create_sequences(scaled_test, N_STEPS)

# X√¢y d·ª±ng m√¥ h√¨nh LSTM
model = Sequential([
    LSTM(50, return_sequences=True, input_shape=(N_STEPS, X_train.shape[2])),
    Dropout(0.2),
    LSTM(50),
    Dropout(0.2),
    Dense(1, activation='sigmoid')  # Ph√¢n lo·∫°i nh·ªã ph√¢n (gi·ªù cao ƒëi·ªÉm hay kh√¥ng)
])

# Compile m√¥ h√¨nh
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model.summary()

# Hu·∫•n luy·ªán m√¥ h√¨nh
print("üöÄ Training model...")
history = model.fit(X_train, y_train, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(X_val, y_val))

# ƒê√°nh gi√° tr√™n t·∫≠p test
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f"‚úÖ Test Accuracy: {test_acc:.4f}, Test Loss: {test_loss:.4f}")

# L∆∞u m√¥ h√¨nh
model_path = os.path.join(MODEL_DIR, "lstm_traffic_model.keras")
model.save(model_path)
print(f"üíæ Model saved to {model_path}")

