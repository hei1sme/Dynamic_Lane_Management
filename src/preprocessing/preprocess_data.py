import pandas as pd
import numpy as np
import os
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
import joblib  # ƒê·ªÉ l∆∞u scaler

# ƒê·ªãnh nghƒ©a th∆∞ m·ª•c ƒë·ªông
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
RAW_DATA_DIR = os.path.join(BASE_DIR, "data", "raw")
PROCESSED_DATA_DIR = os.path.join(BASE_DIR, "data", "processed")
os.makedirs(PROCESSED_DATA_DIR, exist_ok=True)  # ƒê·∫£m b·∫£o th∆∞ m·ª•c t·ªìn t·∫°i

# Load dataset
def load_data(file_path):
    if not os.path.exists(file_path):
        raise FileNotFoundError(f"‚ùå File {file_path} kh√¥ng t·ªìn t·∫°i!")
    return pd.read_csv(file_path)

# X·ª≠ l√Ω missing values
def handle_missing_values(df):
    num_cols = df.select_dtypes(include=[np.number]).columns
    for col in num_cols:
        if df[col].isna().sum() > 0:
            df[col].fillna(df[col].median(), inplace=True)

    cat_cols = df.select_dtypes(exclude=[np.number]).columns
    for col in cat_cols:
        df[col].fillna("Unknown", inplace=True)
    
    return df

# Lo·∫°i b·ªè duplicates
def remove_duplicates(df):
    return df.drop_duplicates()

# X·ª≠ l√Ω outliers (clip gi√° tr·ªã)
def handle_outliers(df, columns):
    for col in columns:
        if col in df.columns:  # Ki·ªÉm tra n·∫øu c·ªôt t·ªìn t·∫°i
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            # Ki·ªÉm tra n·∫øu lower_bound < upper_bound tr∆∞·ªõc khi th·ª±c hi·ªán clipping
            if lower_bound < upper_bound:
                df[col] = np.clip(df[col], lower_bound, upper_bound)
            else:
                print(f"‚ö†Ô∏è C·ªôt {col} c√≥ gi√° tr·ªã b·∫•t th∆∞·ªùng, kh√¥ng th·ª±c hi·ªán clipping.")
    return df

# Chu·∫©n h√≥a d·ªØ li·ªáu (s·ª≠ d·ª•ng scaler ƒë√£ l∆∞u)
def normalize_data(df, columns, scaler_path):
    scaler = MinMaxScaler()
    # Ki·ªÉm tra n·∫øu scaler ƒë√£ t·ªìn t·∫°i, n·∫øu c√≥ th√¨ load scaler ƒë√£ l∆∞u
    if os.path.exists(scaler_path):
        scaler = joblib.load(scaler_path)
        df[columns] = scaler.transform(df[columns])  # S·ª≠ d·ª•ng transform cho d·ªØ li·ªáu m·ªõi
    else:
        df[columns] = scaler.fit_transform(df[columns])
        joblib.dump(scaler, scaler_path)  # L∆∞u scaler ƒë·ªÉ d√πng cho val/test
    return df

# One-Hot Encoding v·ªõi ki·ªÉm tra gi√° tr·ªã kh√¥ng x√°c ƒë·ªãnh
def encode_categorical(df, columns, encoder_dict):
    for col in columns:
        if col in df.columns and df[col].nunique() > 1:  # Ki·ªÉm tra t·ªìn t·∫°i v√† c√≥ gi√° tr·ªã kh√°c nhau
            # Thay th·∫ø gi√° tr·ªã 'Unknown' ho·∫∑c gi√° tr·ªã kh√¥ng x√°c ƒë·ªãnh v·ªõi gi√° tr·ªã m·∫∑c ƒë·ªãnh
            df[col].fillna('Unknown', inplace=True)
            encoder = OneHotEncoder(sparse_output=False, drop="first")
            encoded = encoder.fit_transform(df[[col]])
            encoded_df = pd.DataFrame(encoded, columns=encoder.get_feature_names_out([col]))
            df = df.drop(col, axis=1).reset_index(drop=True)
            df = pd.concat([df, encoded_df], axis=1)
            
            # L∆∞u encoder ƒë·ªÉ √°p d·ª•ng sau n√†y
            encoder_dict[col] = encoder
    return df, encoder_dict

# H√†m ch√≠nh
def preprocess_data(file_name):
    file_path = os.path.join(RAW_DATA_DIR, file_name)
    print(f"üì• Loading file: {file_path}")

    df = load_data(file_path)
    df = handle_missing_values(df)
    df = remove_duplicates(df)
    df = handle_outliers(df, ["num_vehicles", "density_vehicles", "num_stopped_vehicles"])
    
    # Chu·∫©n h√≥a d·ªØ li·ªáu
    scaler_path = os.path.join(PROCESSED_DATA_DIR, "scaler.pkl")
    df = normalize_data(df, ["num_vehicles", "density_vehicles", "num_stopped_vehicles"], scaler_path)

    # One-Hot Encoding
    encoder_dict = {}
    df, encoder_dict = encode_categorical(df, ["traffic_signal_status"], encoder_dict)
    
    # ‚úÖ L∆∞u output v√†o ƒë√∫ng `data/processed`
    output_file = os.path.join(PROCESSED_DATA_DIR, "cleaned_traffic_data.csv")
    df.to_csv(output_file, index=False)
    print(f"‚úÖ Cleaned data saved to: {output_file}")
    
    # L∆∞u c√°c encoder ƒë·ªÉ d√πng cho d·ªØ li·ªáu test v√† validation
    encoder_file = os.path.join(PROCESSED_DATA_DIR, "encoders.pkl")
    joblib.dump(encoder_dict, encoder_file)

    return df, scaler_path, encoder_file

# Ch·∫°y script
if __name__ == "__main__":
    file_name = "synthetic_traffic_data.csv"  # File ƒë·∫ßu v√†o
    cleaned_df, scaler_file, encoder_file = preprocess_data(file_name)
    print(cleaned_df.head())
